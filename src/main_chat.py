import streamlit as st
import requests
from src.api import get_ai_response_stream
from src.chat_utils import create_new_chat, log_ai_activity

COLORS = {
    "bg_user": "#e2e8f0",
    "bg_assistant": "#f1f5f9",
    "border": "#cbd5e1",
    "text": "#1e293b",
    "accent": "#3b82f6",
}


def render_chat_area():
    """Khung chat realtime (bo gÃ³c, mÃ u dá»‹u)."""
    st.header("ğŸ’¬ Chat")

    # --- INIT STATE ---
    for key, val in {
        "is_generating": False,
        "stop_generation": False,
        "activities": [],
    }.items():
        st.session_state.setdefault(key, val)

    if not st.session_state.current_chat_id and not st.session_state.chat_history:
        create_new_chat()

    if not st.session_state.current_chat_id:
        st.info("HÃ£y báº¯t Ä‘áº§u má»™t chat má»›i tá»« sidebar.")
        return

    current_chat = st.session_state.chat_history[st.session_state.current_chat_id]

    # --- HIá»‚N THá»Š Lá»ŠCH Sá»¬ ---
    chat_container = st.container(height=420)
    with chat_container:
        for msg in current_chat["messages"]:
            bg = COLORS["bg_user"] if msg["role"] == "user" else COLORS["bg_assistant"]
            role = "ğŸ‘¤ Báº¡n" if msg["role"] == "user" else "ğŸ¤– Trá»£ lÃ½"
            st.markdown(
                f"""
                <div style="
                    background:{bg};
                    color:{COLORS['text']};
                    border:1px solid {COLORS['border']};
                    border-radius:12px;
                    padding:10px 14px;
                    margin-bottom:10px;">
                    <b>{role}:</b> {msg['content']}
                </div>
                """,
                unsafe_allow_html=True,
            )

    # --- MODEL HIá»†N Táº I ---
    def fetch_current_model():
        try:
            res = requests.get("http://localhost:8000/current-model", timeout=5)
            return res.json().get("model", "KhÃ´ng cÃ³ model") if res.status_code == 200 else "KhÃ´ng cÃ³ model"
        except Exception as e:
            st.warning(f"Lá»—i khi láº¥y model: {e}")
            return "KhÃ´ng cÃ³ model"

    st.session_state.setdefault("selected_model", fetch_current_model())
    st.selectbox("### ğŸ§  Model hiá»‡n táº¡i:", [st.session_state.selected_model], index=0, disabled=True)

    # --- INPUT + STOP ---
    c1, c2 = st.columns([10, 1])
    with c1:
        prompt = st.chat_input("Nháº­p cÃ¢u há»i...", disabled=st.session_state.is_generating)
    with c2:
        if st.session_state.is_generating and st.button("â¹", use_container_width=True, type="primary"):
            st.session_state.stop_generation = True
            st.toast("ğŸ›‘ Dá»«ng tiáº¿n trÃ¬nh")

    if not prompt or st.session_state.is_generating:
        return

    # --- Gá»¬I CÃ‚U Há»I ---
    current_chat["messages"].append({"role": "user", "content": prompt})
    st.session_state.is_generating = True

    with chat_container:
        st.markdown(
            f"""
            <div style="
                background:{COLORS['bg_user']};
                color:{COLORS['text']};
                border:1px solid {COLORS['border']};
                border-radius:12px;
                padding:10px 14px;
                margin-bottom:10px;">
                <b>ğŸ‘¤ Báº¡n:</b> {prompt}
            </div>
            """,
            unsafe_allow_html=True,
        )

    def stream_response():
        full_text = ""
        model_name = None
        for packet in get_ai_response_stream(prompt, st.session_state.selected_model, "chat"):
            if st.session_state.stop_generation:
                log_ai_activity("ğŸ›‘ Dá»«ng tiáº¿n trÃ¬nh", "NgÆ°á»i dÃ¹ng nháº¥n Stop.")
                break

            text = packet.get("content_chunk", "")
            if not text:
                continue

            meta = packet.get("metadata", {})
            node = meta.get("langgraph_node", "?")
            model_name = meta.get("ls_model_name") or st.session_state.selected_model

            full_text += text
            log_ai_activity("ğŸ“¡ Chunk má»›i", f"Node: {node} | Model: {model_name}", meta)
            yield text

        log_ai_activity("âœ… HoÃ n táº¥t tiáº¿n trÃ¬nh", f"MÃ´ hÃ¬nh: {model_name or st.session_state.selected_model}")

    with chat_container:
        st.markdown(
            f"""
            <div style="
                background:{COLORS['bg_assistant']};
                color:{COLORS['text']};
                border:1px solid {COLORS['border']};
                border-radius:12px;
                padding:10px 14px;
                margin-bottom:10px;">
                <b>ğŸ¤– Trá»£ lÃ½:</b> """,
            unsafe_allow_html=True,
        )
        full_reply = st.write_stream(stream_response())
        st.markdown("</div>", unsafe_allow_html=True)

    current_chat["messages"].append({"role": "assistant", "content": full_reply})
    st.session_state.is_generating = False
    st.session_state.stop_generation = False
