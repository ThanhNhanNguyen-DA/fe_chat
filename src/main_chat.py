import streamlit as st
from src.api import get_ai_response_stream
from src.chat_utils import create_new_chat

def render_chat_area():
    """Khu v·ª±c chat ch√≠nh v·ªõi streaming ph·∫£n h·ªìi."""
    st.header("üí¨ Chat")

    # Kh·ªüi t·∫°o state
    state = st.session_state
    for key, default in {
        "is_generating": False,
        "stop_generation": False,
    }.items():
        state.setdefault(key, default)

    # Ki·ªÉm tra chat hi·ªán t·∫°i
    if not state.current_chat_id and not state.chat_history:
        create_new_chat()

    if not state.current_chat_id:
        st.info("H√£y b·∫Øt ƒë·∫ßu chat m·ªõi ·ªü sidebar.")
        return

    chat = state.chat_history[state.current_chat_id]
    chat_container = st.container(height=400)
    with chat_container:
        for msg in chat["messages"]:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    # Ch·ªçn model
    models = get_ai_response_stream()
    selected_model = st.selectbox(
        "Ch·ªçn Model:", models, key="selected_model", disabled=state.is_generating
    )

    # Input v√† Stop
    c1, c2 = st.columns([10, 1])
    with c1:
        prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi...", disabled=state.is_generating)
    with c2:
        if state.is_generating and st.button("‚èπ", use_container_width=True, type="primary"):
            state.stop_generation = True
            st.toast("üõë D·ª´ng ti·∫øn tr√¨nh")
            st.rerun()

    # G·ª≠i c√¢u h·ªèi
    if prompt and not state.is_generating:
        chat["messages"] += [{"role": "user", "content": prompt}, {"role": "assistant", "content": ""}]
        state.is_generating = True
        st.rerun()

    # Nh·∫≠n ph·∫£n h·ªìi
    if state.is_generating and not state.stop_generation:
        with chat_container:
            with st.chat_message("assistant"):
                placeholder = st.empty()
                full, model_name, node_name = "", None, None
                for packet in get_ai_response_stream(prompt, selected_model, activity="chat"):
                    if state.stop_generation:
                        break
                    text = packet.get("content_chunk", "")
                    model_name = packet.get("ls_model_name") or model_name
                    node_name = packet.get("metadata", {}).get("langgraph_node") or node_name
                    full += text
                    placeholder.markdown(
                        f"**Model:** `{model_name}` | **Node:** `{node_name}`\n\n{full}‚ñå"
                    )
                placeholder.markdown(
                    f"**Model:** `{model_name}` | **Node:** `{node_name}`\n\n{full}"
                )
        chat["messages"][-1]["content"] = full
        state.is_generating = state.stop_generation = False
        st.rerun()
