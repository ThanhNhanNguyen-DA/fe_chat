import streamlit as st
from src.api import get_ai_response_stream
from src.chat_utils import create_new_chat
from typing import List, Dict, Any, Optional


# --- 1. Kh·ªüi t·∫°o Session State ---
def initialize_session_state():
    defaults = {
        "is_generating": False,
        "stop_generation": False,
        "feedback_log": {},  # L∆∞u feedback d·∫°ng {index: "positive"/"negative"}
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


# --- 2. L∆∞u feedback ---
def save_feedback(index: int):
    """H√†m callback khi ng∆∞·ªùi d√πng b·∫•m feedback."""
    fb = st.session_state.get(f"feedback_{index}")
    if fb is not None:
        st.session_state.feedback_log[index] = fb
        st.toast(
            "‚ù§Ô∏è C·∫£m ∆°n b·∫°n ƒë√£ g√≥p √Ω!" if fb == "positive"
            else "üí¨ C·∫£m ∆°n ph·∫£n h·ªìi, ch√∫ng t√¥i s·∫Ω c·∫£i thi·ªán."
        )


# --- 3. Hi·ªÉn th·ªã tin nh·∫Øn & feedback ---
def display_chat_messages(messages: List[Dict[str, Any]]):
    """Hi·ªÉn th·ªã to√†n b·ªô l·ªãch s·ª≠ chat c√πng feedback ƒë·∫πp."""
    for i, msg in enumerate(messages):
        # B·ªè qua assistant ƒëang stream (r·ªóng)
        if msg["role"] == "assistant" and msg.get("content", "").strip() == "" and st.session_state.is_generating:
            continue

        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

            # Ch·ªâ hi·ªÉn th·ªã feedback cho assistant
            if msg["role"] == "assistant" and msg["content"].strip():
                fb_value = st.session_state.feedback_log.get(i)
                st.session_state[f"feedback_{i}"] = fb_value

                st.feedback(
                    "thumbs",
                    key=f"feedback_{i}",
                    disabled=fb_value is not None,
                    on_change=save_feedback,
                    args=(i,),
                )


# --- 4. Hi·ªÉn th·ªã ph·∫ßn ch·ªçn model & input ---
def display_chat_controls() -> tuple[Optional[str], str]:
    try:
        model_options = get_ai_response_stream()
    except Exception as e:
        st.error(f"L·ªói khi l·∫•y danh s√°ch model: {e}")
        model_options = ["default-model"]

    selected_model = st.selectbox(
        "Ch·ªçn Model:",
        model_options,
        disabled=st.session_state.is_generating,
    )

    c1, c2 = st.columns([10, 1])
    with c1:
        prompt = st.chat_input("Nh·∫≠p c√¢u h·ªèi...", disabled=st.session_state.is_generating)
    with c2:
        if st.session_state.is_generating:
            if st.button("‚èπ", use_container_width=True, type="primary"):
                st.session_state.stop_generation = True
                st.toast("üõë D·ª´ng")
                st.rerun()

    return prompt, selected_model


# --- 5. Logic g·ª≠i prompt ---
def handle_prompt_submission(prompt: str, current_chat: Dict[str, Any]):
    current_chat["messages"].append({"role": "user", "content": prompt})
    current_chat["messages"].append({"role": "assistant", "content": ""})
    st.session_state.is_generating = True


# --- 6. Stream ph·∫£n h·ªìi ---
def stream_and_display_response(current_chat: Dict[str, Any], selected_model: str):
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        model_name = node_name = None
        user_prompt = current_chat["messages"][-2]["content"]

        try:
            stream = get_ai_response_stream(user_prompt, selected_model, activity="chat")

            for packet in stream:
                if st.session_state.stop_generation:
                    st.toast("ƒê√£ d·ª´ng")
                    break

                text_chunk = packet.get("content_chunk", "")
                model_name = packet.get("ls_model_name") or model_name
                meta = packet.get("metadata", {})
                node_name = meta.get("langgraph_node") or node_name

                full_response += text_chunk
                header = f"**Model:** `{model_name}` | **Node:** `{node_name}`" if model_name and node_name else ""
                placeholder.markdown(f"{header}\n\n{full_response}‚ñå")

            header = f"**Model:** `{model_name}` | **Node:** `{node_name}`" if model_name and node_name else ""
            placeholder.markdown(f"{header}\n\n{full_response}")

            # ‚úÖ Hi·ªÉn th·ªã feedback ‚Äúthumbs‚Äù ngay d∆∞·ªõi ph·∫£n h·ªìi cu·ªëi
            st.feedback(
                "thumbs",
                key=f"feedback_{len(current_chat['messages'])}",
                on_change=save_feedback,
                args=(len(current_chat["messages"]),),
            )

        except Exception as e:
            st.error(f"L·ªói khi streaming ph·∫£n h·ªìi: {e}")
            full_response = "ƒê√£ x·∫£y ra l·ªói, vui l√≤ng th·ª≠ l·∫°i."
            placeholder.markdown(full_response)
        finally:
            current_chat["messages"][-1]["content"] = full_response
            st.session_state.is_generating = False
            st.session_state.stop_generation = False


# --- 7. H√†m ch√≠nh ---
def render_chat_area():
    st.header("üí¨ Chat")

    initialize_session_state()

    if not st.session_state.get("current_chat_id") and not st.session_state.get("chat_history"):
        create_new_chat()

    if not st.session_state.get("current_chat_id"):
        st.info("H√£y b·∫Øt ƒë·∫ßu m·ªôt chat m·ªõi t·ª´ sidebar.")
        return

    current_chat = st.session_state.chat_history[st.session_state.current_chat_id]

    chat_container = st.container(height=400)
    with chat_container:
        display_chat_messages(current_chat["messages"])

    prompt, selected_model = display_chat_controls()

    if prompt:
        handle_prompt_submission(prompt, current_chat)
        st.rerun()

    if st.session_state.is_generating and not st.session_state.stop_generation:
        with chat_container:
            stream_and_display_response(current_chat, selected_model)
        st.rerun()
